{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.image as img\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.image as img\n",
    "from keras.preprocessing.image import ImageDataGenerator, Iterator, flip_axis\n",
    "from keras.layers.core import Dense, Flatten, Activation, SpatialDropout2D, Lambda, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.objectives import mse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# 1. Read Data from Driving Log\n",
    "driving_log = pd.read_csv('driving_log.csv', index_col=False)\n",
    "driving_log.columns = ['center_imgpath', 'left_imgpath', 'right_imgpath', 'angle', 'throttle', 'break', 'speed']\n",
    "\n",
    "# 2. Prepare Data for Generator\n",
    "X_train_path, y_tr = [], []\n",
    "for index, row in driving_log.iterrows():\n",
    "    # Include center image and steering angle.\n",
    "    \n",
    "    flip_a_coin = np.random.randint(3)\n",
    "    if flip_a_coin!=2:\n",
    "        X_train_path.append(row['center_imgpath'])\n",
    "        y_tr.append(row['angle'])\n",
    "           \n",
    "    C = row['angle']\n",
    "    L = C + 0.20\n",
    "    R = C - 0.20\n",
    "        \n",
    "    X_train_path.append(row['left_imgpath'].strip())\n",
    "    y_tr.append(L)\n",
    "        \n",
    "    X_train_path.append(row['right_imgpath'].strip())\n",
    "    y_tr.append(R)\n",
    "\n",
    "      \n",
    "X_train_path, y_tr = np.array(X_train_path), np.array(y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TARGET_SIZE = (64, 64)\n",
    "\n",
    "def read_imgs(img_paths):\n",
    "    \"\"\"\n",
    "    read images from file path.\n",
    "    \"\"\"        \n",
    "    imgs = np.empty([len(img_paths), 64, 64, 3])\n",
    "\n",
    "    for i, path in enumerate(img_paths):\n",
    "        imgs[i] = preprocess_image(imread(path))\n",
    "\n",
    "    return imgs\n",
    "\n",
    "def resize_to_target_size(image):\n",
    "    return cv2.resize(image, TARGET_SIZE)\n",
    "\n",
    "\n",
    "def crop_and_resize(image):\n",
    "    '''\n",
    "    :param image: The input image of dimensions 160x320x3\n",
    "    :return: Output image of size 64x64x3\n",
    "    '''\n",
    "    cropped_image = image[55:135, :, :]\n",
    "    processed_image = resize_to_target_size(cropped_image)\n",
    "    return processed_image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = crop_and_resize(image)\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    #Normalize image\n",
    "    image = image/255.0 - 0.5\n",
    "    return image\n",
    "        \n",
    "def get_model():\n",
    "    \n",
    "    model = Sequential([\n",
    "\n",
    "        # Conv 5x5\n",
    "        Convolution2D(24, 5, 5, border_mode='same', activation='elu', input_shape=(64, 64, 3)),\n",
    "        MaxPooling2D(border_mode='same'),\n",
    "        # Conv 5x5\n",
    "        Convolution2D(36, 5, 5, border_mode='same', activation='elu'),\n",
    "        MaxPooling2D(border_mode='same'),\n",
    "        # Conv 5x5\n",
    "        Convolution2D(48, 5, 5, border_mode='same', activation='elu'),\n",
    "        MaxPooling2D(border_mode='same'),\n",
    "        # Conv 3x3\n",
    "        Convolution2D(64, 3, 3, border_mode='same', activation='elu'),\n",
    "        MaxPooling2D(border_mode='same'),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # Conv 3x3\n",
    "        Convolution2D(64, 3, 3, border_mode='same', activation='elu'),\n",
    "        MaxPooling2D(border_mode='same'),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # Flatten\n",
    "        Flatten(),\n",
    "        # Fully Connected\n",
    "        Dense(100, activation='elu'),\n",
    "        Dense(50, activation='elu'),\n",
    "        Dense(10, activation='elu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def gen_batches(imgs, angles, batch_size, flip_prob=0.5):\n",
    "    \"\"\"\n",
    "    Generates random batches of the input data.\n",
    "    :imgs: The input image file paths\n",
    "    :angles: The steering angles associated with each image.\n",
    "    :batch_size: The size of each minibatch.\n",
    "    :yeilds A tuple (images, angles), where both images and angles have batch_size elements.\n",
    "    \"\"\"\n",
    "    num_imgs = len(imgs)\n",
    "\n",
    "    while True:\n",
    "        indices = np.random.choice(num_imgs, batch_size)\n",
    "        batch_imgs, angles_raw = read_imgs(imgs[indices]), angles[indices].astype(float)\n",
    "        \n",
    "        # Add random flipping for images\n",
    "        for i in range(batch_size):\n",
    "            prob = np.random.random()\n",
    "            if prob<flip_prob:\n",
    "                batch_imgs[i] = cv2.flip(batch_imgs[i],1)\n",
    "                angles_raw[i] = -angles_raw[i]\n",
    " \n",
    "        \n",
    "        \n",
    "        yield batch_imgs, angles_raw\n",
    "        \n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    with open('model.json', 'w') as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    model.save_weights('model.h5')\n",
    "\n",
    "    print ('Model saved!')\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "12800/12800 [==============================] - 24s - loss: 0.0046 - val_loss: 0.0029\n",
      "Epoch 2/4\n",
      "12800/12800 [==============================] - 20s - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 3/4\n",
      "12800/12800 [==============================] - 20s - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 4/4\n",
      "12800/12800 [==============================] - 20s - loss: 0.0027 - val_loss: 0.0026\n",
      "Model saved!\n",
      "Training completed in 137.301656s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "model = get_model()\n",
    "#plot(model, to_file='model.png', show_shapes=True)\n",
    "model.compile(optimizer='adam', loss = 'mse')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_path, y_tr,\n",
    "                                                  test_size=0.10,\n",
    "                                                  random_state = 0)\n",
    "\n",
    "#Define Optimizer & Loss Function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # loss is proportional to turning angle to reduce bias of straight paths\n",
    "    return mse(y_true, y_pred) * np.absolute(y_true)\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "model.fit_generator(gen_batches(X_train, y_train, batch_size),\n",
    "    samples_per_epoch=12800,\n",
    "    validation_data=gen_batches(X_val,y_val, batch_size),\n",
    "    nb_val_samples=len(y_val),\n",
    "    nb_epoch=4\n",
    ")\n",
    "\n",
    "# Save Model/Weights\n",
    "save_model(model)\n",
    "\n",
    "end = time.clock()\n",
    "print (\"Training completed in {:2f}s\".format(end - start))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:sdc]",
   "language": "python",
   "name": "conda-env-sdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
